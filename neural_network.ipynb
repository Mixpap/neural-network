{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load 'neural_network.py'\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import random\n",
    "\n",
    "def softmax(w):\n",
    "    return np.exp(w) / np.sum(np.exp(w))\n",
    "\n",
    "\n",
    "def activation_function(case):\n",
    "    \"\"\"\n",
    "    Returns the corresponding activation function and the derrivative of\n",
    "    this function\n",
    "    :param case: The number of the activation function\n",
    "    :return: the activation function to be used by the neurons\n",
    "     in the hidden layer\n",
    "    \"\"\"\n",
    "    # h(a) = log(1 + exp^a)\n",
    "    # h(a) = (exp^a - exp^(-a))/(exp^a + exp^(-a))\n",
    "    # h(a) = cos(a)\n",
    "    def logarithmic(a):\n",
    "        \"\"\"\n",
    "        programmed like this for numerical stability\n",
    "        :param a:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        m = np.max(0,a);#CHECK\n",
    "        return m + np.log( np.exp(-m) + np.exp(a-m)), 1/1+np.exp(-a)\n",
    "    def tanh(a):\n",
    "        return (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a)), 1- (np.exp(\n",
    "            a)-np.exp(-a))/(np.exp(a)+np.exp(-a))**2\n",
    "    def cosine(a):\n",
    "        return np.cos(a), -np.sin(a)\n",
    "    if case == 1:\n",
    "        return logarithmic\n",
    "    elif case == 2:\n",
    "        return tanh\n",
    "    elif case == 3:\n",
    "        return cosine\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, x_train, hidden_layer_activation_function,\n",
    "                 hidden_neurons,\n",
    "                 number_of_outputs, lamda, iter, t, eta,tol,\n",
    "                 hidden_bias=None,\n",
    "                 output_bias=None):\n",
    "        self.iter = iter\n",
    "        self.x_train = x_train\n",
    "        self.t = t\n",
    "        self.lamda = lamda\n",
    "        self.eta = eta\n",
    "        self.tol = tol\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.hidden_layer = Layer(hidden_neurons, hidden_bias)\n",
    "        self.output_layer = Layer(number_of_outputs, output_bias)\n",
    "        self.hidden_activation, self.grad_activation = activation_function(\n",
    "            hidden_layer_activation_function)\n",
    "        #initialize weights\n",
    "        self.w1 = np.random.randn(hidden_neurons, np.size(input,0)+1)\n",
    "        self.w2 = np.random.randn(number_of_outputs, hidden_neurons+1)\n",
    "\n",
    "    def forward_prop(self, x, t, w1, w2):\n",
    "        \"\"\"\n",
    "        feed forward and get error\n",
    "        sum(sum( T.*Y )) - sum(M)  - sum(log(sum(exp(Y - repmat(M, 1, K)), 2)))\n",
    "          - (0.5*lambda)*sum(sum(W2.*W2));\n",
    "        :param x:\n",
    "        :param t:\n",
    "        :param w1:\n",
    "        :param w2:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #feed forward\n",
    "        z = self.hidden_activation(x*w1) #check matrix\n",
    "        #add bias to z\n",
    "        z_with_bias = np.ones((np.size(z,1),np.size(z,0)+1))\n",
    "        z_with_bias[:,1:] = z\n",
    "        # multiplication and transpose\n",
    "        y = z*w2 #check transpose(normaly w2)\n",
    "        max_error = np.max(y, 1)\n",
    "        E = np.sum(np.sum(np.dot(t, y)) - np.sum(max_error)- np.sum(np.log(\n",
    "            np.sum(np.exp(y-np.matlib.repmat(max_error, 1,\n",
    "                                             self.number_of_outputs)),\n",
    "                   2)))-(1/2*self.lamda)*np.sum(np.sum(w2**2)))\n",
    "        s = softmax(y)\n",
    "        gradw2 = np.transpose((t-s))*z_with_bias - self.lamda*w2\n",
    "        #get rid of the bias\n",
    "        w2 = w2[:, 2:]\n",
    "        gradw1 = w2*np.transpose(t-s)*self.grad_activation(x*w1)*x\n",
    "        return E, gradw1, gradw2\n",
    "\n",
    "    def train(self):\n",
    "        e_old = -np.inf\n",
    "        for i in range(iter):\n",
    "            e, gradw1, gradw2 = self.forward_prop(self.x_train, self.t,\n",
    "                                                  self.w1,\n",
    "                                             self.w2)\n",
    "            print i,\" iteration cost =\",e\n",
    "            if abs(e - e_old) < self.tol:\n",
    "                break\n",
    "            self.w1 = self.w1 + self.eta*gradw1\n",
    "            self.w2 = self.w2 + self.eta*gradw2\n",
    "            e_old = e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, num_of_neurons, bias=None):\n",
    "        self.num_of_neurons = num_of_neurons\n",
    "        self.neurons = [random.random() for i in range(num_of_neurons)]\n",
    "        self.bias = bias\n",
    "\n",
    "    def get_info(self):\n",
    "        print ', '.join(\"%s: %s\" % item for item in vars(self).items())\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weights):\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_info(self):\n",
    "        print ', '.join(\"%s: %s\" % item for item in vars(self).items())\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
